Product Requirements Document
Product Name: AI-Powered Student Repo Grader
 Version: 1.0
 Prepared by: Kidus Elias
 Date: April 23, 2025

1. Overview
The AI-Powered Student Repo Grader is a web-based application that automates the grading of coding assignments submitted by students via GitHub repositories. The tool uses a grading rubric provided by the instructor and analyzes student code submissions to output structured grading reports. It flags inaccessible repositories, evaluates code against a defined rubric, estimates the percentage of AI-generated code, and outputs a detailed grading spreadsheet.

2. Goals and Success Criteria
2.1 Goals
Automate the evaluation of student code submissions from GitHub.


Ensure consistency and objectivity in grading through rubric-based assessment.


Detect and flag inaccessible or invalid repository links.


Identify and report unusual patterns or potential plagiarism (including AI-generated code).


2.2 Success Criteria
Accurate and interpretable grading output for each student repository.


Clear, downloadable spreadsheet of all evaluations.


Usability for instructors with minimal technical intervention.


Process completion for at least 95% of valid repositories within 10 minutes for a batch of 30 students.



3. Core Features
3.1 CSV Upload Interface
Users can upload a CSV file containing two columns:


Student Name


GitHub Repository URL


CSV should support UTF-8 encoding.


File size limit: 5MB.


3.2 Rubric Upload Interface
Users upload a structured assessment rubric.


Supported formats:


JSON (preferred for structured parsing)


Markdown or PDF (optional, requires AI parsing)


Rubric must define:


Criterion name


Maximum score per criterion


Evaluation description or guidelines per criterion


3.3 Repository Crawler
Automatically clones each GitHub repository.


Uses GitHub‚Äôs API or git CLI to validate accessibility.


If inaccessible (e.g., private or non-existent), log an error with:


Error type (404, permission denied, etc.)


Affected student‚Äôs name and repository URL


Repos are analyzed for structure, file count, commit history, and code patterns.


3.4 AI Grading Engine
Parses student code and evaluates against rubric.


Scores each rubric criterion based on code analysis, structure, logic, and documentation.


Outputs:


Score per criterion


Short justification for each score


Total score out of maximum


Summary note on the submission‚Äôs strengths, weaknesses, and any red flags


3.5 AI-Generated Code Estimation
Estimates what percentage of code was likely written by AI tools (e.g., ChatGPT, Copilot).


Techniques may include:


Semantic similarity to known AI-generated patterns


Commit message patterns


Authorship anomalies


Integration of external models (e.g., GPTZero API)


3.6 Outlier Detection
Flags submissions with:


Unusually high or low scores


Unusual patterns (e.g., no commit history, all files created within minutes)


Sudden spikes in performance relative to past submissions


3.7 Report Generation
Outputs a downloadable spreadsheet (CSV and XLSX formats) with the following columns:


Student Name


Repository URL


Criteria 1 Score


Criteria 2 Score


... (one column per rubric criterion)


Total Score


AI Code Estimate (%)


Evaluation Note


Issues or Flags



4. User Stories
4.1 CSV Upload
As an instructor, I want to upload a CSV of student names and GitHub URLs so that I can evaluate all submissions in bulk.
4.2 Rubric-Based Grading
As an instructor, I want to upload a rubric that defines how student projects should be graded so that grading is consistent and transparent.
4.3 Code Evaluation
As the system, I want to automatically analyze each GitHub repository so that instructors do not need to grade manually.
4.4 Inaccessible Repo Handling
As the system, I want to flag repositories that are inaccessible so instructors can follow up with affected students.
4.5 AI Authorship Estimation
As an instructor, I want to estimate how much of the code was generated by AI so I can better evaluate the originality of submissions.
4.6 Reporting
As an instructor, I want to download a detailed spreadsheet that includes scores, AI authorship estimates, and notes so that I can review or share results efficiently.

5. Inputs and Outputs
5.1 Inputs
CSV File:


Required columns: Student Name, GitHub URL


Rubric File:


JSON with structure:

 json
CopyEdit
{
  "criteria": [
    {
      "name": "Code Structure",
      "max_points": 20,
      "description": "Evaluate the organization and modularity of the code"
    },
    {
      "name": "Functionality",
      "max_points": 30,
      "description": "Check if the project runs and meets the specifications"
    }
  ]
}


5.2 Outputs
.csv or .xlsx file containing:


Student Name


GitHub URL


Criteria Scores


Total Score


AI-written Code Estimate


Summary Note


Flags or Issues



6. Non-Functional Requirements
Web application should support batch uploads (up to 100 students).


Asynchronous processing with email or notification once grading is complete.


Use secure temporary storage for uploaded documents and repository clones.


No student data is stored permanently unless explicitly enabled by the user.


Application response time should not exceed 3 seconds per page interaction.



7. Edge Cases and Error Handling
Scenario
Handling Strategy
The GitHub repo is private
Log and flag in the output with appropriate message
Rubric format is invalid
Display error with required formatting example
Repo contains no code
Assign zero, flag as 'empty repository'
Long-running repo analysis
Use background jobs with timeout fallback
Mixed languages in project
Focus on primary language inferred from files


8. Future Enhancements (Optional)
Plagiarism detection between student submissions


GitHub OAuth integration for repo access


UI for rubric creation and editing


Visualization of class-wide performance distribution




9. Tech stack to be used
üë®‚Äçüíª Backend: Python + FastAPI
FastAPI ‚Äì Super fast, async-ready, perfect for building a RESTful backend in record time.


GitPython ‚Äì To clone and interact with student GitHub repositories.


OpenAI API / Transformers (HuggingFace) ‚Äì For detecting AI-generated code (e.g., using GPTZero, CodeBERT, or similar models).


Pydantic ‚Äì For data validation and handling grading rubrics cleanly.


Pandas ‚Äì For processing grading data and generating CSV or Excel reports.


üß† AI Code Analysis:
Use a pretrained model or heuristic-based detection for AI-generated code (can explore GPT detectors like detectGPT, GPTZero, or custom fine-tuned classifiers).


Compare code quality, structure, docstrings, etc., with the rubric.


üóÇÔ∏è Rubric Handling:
Store the rubric in JSON/YAML format (flexible, easy to edit).


Parse and match against code metrics (e.g., function naming, comments, modularity, test cases).


üìÇ GitHub Integration:
GitHub API (via PyGitHub) ‚Äì To check repo access, contributors, pull files, etc.


Handle token auth for accessing private repos.


üìà Report Generation:
Output structured reports as:


CSV or Excel files via pandas


PDF (optional) via reportlab or WeasyPrint


üåê Frontend (Optional MVP):
If you want a frontend:
React + Tailwind CSS for a sleek dashboard.


If in a rush, keep it minimal or even use Jinja2 + FastAPI templating.


üß™ Testing & Linting:
pytest, black, flake8 ‚Äì for clean and testable code.


‚òÅÔ∏è Deployment:
Render / Railway / Fly.io ‚Äì Easiest Python backend hosting.


CI/CD: GitHub Actions for automating tests and deployments.

