""" this file is responsible for assessing the AI generated code. so based on the given rubric, it will assess the code and return the score the data will be used to generate a report and also we will get the name of the student and the girhub url from csv.py and the analyzed codes from repo_analyzer.py we will use gpt3.5-turbo"""

import os
import json
import requests
import pandas as pd
from repo_analyzer import analyze_repo
# Removed unused import of read_csv from csv_analyzer
from rubric_handler import load_rubric
import openai
import dotenv

dotenv.load_dotenv()

try:
    client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    print(f"Error: {e} check your environment for openai key")
    exit(1)

def assess_code(code, rubric, client):
    """
    Assesses the given code based on the provided rubric using OpenAI API.
    """
    if not rubric:
        rubric = load_rubric() # Load default if none provided

    # Ensure rubric is formatted as a string list if it's not already
    if isinstance(rubric, list):
        rubric_str = "\n".join([f"- {item}" for item in rubric])
    else:
        rubric_str = str(rubric) # Assume it's already a string

    system_prompt = """
You are a meticulous and strict Code Quality Assessor. Your task is to evaluate the provided code based *strictly* on the given rubric criteria. Perform a detailed analysis of the code's structure, logic, style, and adherence to best practices relevant to the rubric.

Your response MUST follow this structure:

1.  **Rubric-Based Assessment:**
    *   For *each* criterion in the rubric, provide a detailed evaluation with specific examples from the code.
    *   Clearly state how well the code meets each criterion.

2.  **Plagiarism Check:**
    *   Analyze the code for potential plagiarism. State your findings (e.g., "Plagiarism: None detected", "Plagiarism: Potential similarities found with [source/pattern]").

3.  **AI-Generated Code Check:**
    *   Assess the likelihood that the code was generated by an AI. Provide your reasoning (e.g., "AI Generated: Unlikely. Code shows typical human developer patterns.", "AI Generated: Likely. Contains patterns common in AI outputs.").

4.  **Overall Score:**
    *   Provide a single, overall score out of 10, reflecting the code's quality based *only* on the rubric evaluation.
    *   The score *must* be present in the format: "Overall Score: X/10". Ensure X is a number between 0 and 10.

5.  **Overall Summary:**
    *   Provide a brief summary of the code's strengths and weaknesses based on your assessment.

Be thorough, objective, and ensure the score is always included.
"""

    user_prompt = f"""
**Rubric:**
{rubric_str}

---

**Code to Assess:**
```
{code}
```
---

Please provide your assessment based strictly on the rubric and instructions. Ensure the "Overall Score: X/10" line is present.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o", # Using a more capable model like gpt-4o might yield better results for detailed analysis
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )
        assessment_content = response.choices[0].message.content

        # Basic check to see if the score format is likely present
        if "Overall Score:" not in assessment_content:
             # Fallback or retry logic could be added here if needed
             print("Warning: 'Overall Score:' format might be missing in the response.")
             # You could potentially append a default score or re-prompt

        return assessment_content
    except Exception as e:
        print(f"Error during OpenAI API call: {e}")
        # Return a structured error message or default assessment
        return f"Error assessing code: {e}\n\nOverall Score: 0/10"

def main():
    csv_data = read_csv()
    rubric = load_rubric()
    scores = []
    for row in csv_data.itertuples():
        analyzed_code = analyze_repo(row.github_url)
        score = assess_code(analyzed_code, rubric, client)
        scores.append({
            "name": row.name,
            "github_url": row.github_url,
            "score": score
        })
    df = pd.DataFrame(scores)
    df.to_csv("scores.csv", index=False)
